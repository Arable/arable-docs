{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Once data is stored as a dataframe, there are many nuts-and-bolts operations to add new columns, drop columns, resample data, aggregate data, join multiple tables, concatenate rows, and the like.  \n",
    "\n",
    "We use the [pandas](https://pandas.pydata.org/) framework for handling data, chiefly because it has a lot of built-in functions for handling data like ours, which is to say time-indexed multivariate data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with indices\n",
    "\n",
    "One of the main advantages of using a framework like pandas is the ability to index the data, which essentially formalizes your intuition that data is organized as a timeseries with some number of columns.  With that index on time, you can join this table with other tables indexed on time, _even if they have different timestamps_.  Pandas has useful functions to resample to a consistent time interval (for example to go from 3-hourly to 1-hourly or 1-hourly to daily), and along with this come rules for what to do during that resampling or aggregating.  \n",
    "\n",
    "A few applications we'll look at here are\n",
    "\n",
    "* Setting the index to local time using the time offset from `location`.  You may use this when comparing Mark data to some local data source.\n",
    "\n",
    "* Calculating daily maximum, minimum, average, and other such metrics from hourly data.\n",
    "\n",
    "* Joining hourly data (such as reference evapotranspiration or ET) with daily data (such as a crop coefficient) to calculate crop ET.  \n",
    "\n",
    "First things first let's import the modules we need and download a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "from io import StringIO\n",
    "from arable.client import ArableClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ArableClient()\n",
    "a.connect(arable_email, arable_passwd, arable_tenant)\n",
    "\n",
    "device = 'A000176' \n",
    "\n",
    "sta = \"2018-07-04 08:00:00\"\n",
    "end = \"2018-07-07 08:00:00\"\n",
    "\n",
    "hourly = a.query(select='all', \n",
    "             format='csv', \n",
    "             devices=[device], \n",
    "             measure='hourly', \n",
    "             order='time', \n",
    "             end=end, start=sta) \n",
    "\n",
    "hourly = StringIO(hourly)\n",
    "hourly = pd.read_csv(hourly, sep=',', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next order of business is to set the `time` column as datetime object as python understands them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly['time'] = pd.to_datetime(hourly['time'])\n",
    "hourly.index = hourly['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at this dataframe we now see that it is indexed.  If we do anything to this dataframe, like create a new column (perhaps as a calculation from existing columns) it will preserve the integrity of the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our time is stored as UTC, where the \"U\" stands for Universal.  It is a format that is easy for machines.  But humans tend to prefer local time. We saw in [example 1](https://pro-soap.cloudvent.net/ex1_Downloading.html) that it was possible to grab the local time offset from the `locations` data structure associated with the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_token = a.header['Authorization']\n",
    "location_id = a.devices(name='A000176')['location']['id']\n",
    "\n",
    "base = 'https://api-user.arable.cloud/api/v1/'\n",
    "path = '/locations/'\n",
    "url = base + path + location_id\n",
    "\n",
    "response = requests.get(url, headers = {'Authorization': auth_token})\n",
    "location = response.json()\n",
    "\n",
    "time_offset = location['time_offset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `timedelta` to apply the time offset to the UTC time, adding a column using `['column_name']` syntax.  We then set it as our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly['local_time'] = hourly['time'] + timedelta(seconds=time_offset)\n",
    "hourly.index = hourly['local_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, any aggregation or resampling (such as daily max and min) will be performed using local time instead of UTC time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner join\n",
    "\n",
    "One of the useful features of using indexed dataframes is the ease of joining tables.  The join always happens using a unique identifier (UID) that both tables have in common.  For us, the obvious UID is the time index.  \n",
    "\n",
    "In database world there are two basic joins: the \"inner join\" and the \"outer join\".  And within outer joins there are \"left outer joins\" and \"right outer joins\".  To explain, let's imagine two tables (A and B) with the indices shown:\n",
    "\n",
    "```\n",
    "A B\n",
    "1 1\n",
    "2 \n",
    "3 3\n",
    "  4\n",
    "5 5\n",
    "```\n",
    "\n",
    "An _inner join_ returns only the rows where the index is present in both tables:\n",
    "\n",
    "```\n",
    "AB\n",
    "1\n",
    "3\n",
    "5\n",
    "```\n",
    "\n",
    "A _left outer join_ returns all the rows where the left table has an index, and inserts blank values where the right table has nothing:\n",
    "\n",
    "```\n",
    "AB\n",
    "1\n",
    "2\n",
    "3\n",
    "5\n",
    "```\n",
    "\n",
    "Similarly for a right outer join:\n",
    "\n",
    "```\n",
    "AB\n",
    "1\n",
    "3\n",
    "4\n",
    "5\n",
    "```\n",
    "\n",
    "One common join for us is to join the `health` table to the `hourly` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "health = a.query(select='all', \n",
    "             format='csv', \n",
    "             devices=[device], \n",
    "             measure='health', \n",
    "             order='time', \n",
    "             end=end, start=sta) \n",
    "\n",
    "health = StringIO(health)\n",
    "health = pd.read_csv(health, sep=',', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly['time'] = pd.to_datetime(hourly['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One \"gotcha\" on joins: the time must be _exactly_ the same.  If you are joining Mark data with some other table (an image collected from a satellite or sample from a handheld device, for example), you need to handle this.  For us, we'll keep it simple and just strip off the minutes and seconds, because we only collect health data hourly when we sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "health.['time'] = health['time'].replace(minute=0).replace(second=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the local time and set it as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "health['local_time'] = health['time'] + timedelta(seconds=time_offset)\n",
    "health.index = health['local_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "And now the join is accomplished with the call to `merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(left=hourly,right=health,on='local_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "```\n",
    "mask = (sdf['solartime'] >= start) \n",
    "idf = sdf.loc[mask].copy()\n",
    "```\n",
    "\n",
    "## Dropping columns\n",
    "\n",
    "## Outer join, forward fill (e.g. for a custom kc)\n",
    "\n",
    "```\n",
    "tdf = tdf.index.normalize()\n",
    "tdf = tdf.merge(sdf, how='outer', sort=True)\n",
    "tdf = tdf.fillna(method='ffill')\n",
    "```\n",
    "\n",
    "## Diurnal temperature range\n",
    "\n",
    "\n",
    "## Groupby, resample, sum to get daily or weekly metrics\n",
    "https://stackoverflow.com/questions/45281297/group-by-week-in-pandas\n",
    "\n",
    "```\n",
    "df = df.resample('30S').ffill().resample('5T').mean()\n",
    "```\n",
    "\n",
    "```\n",
    "tdf = tdf.groupby(pd.Grouper(freq='D')).min()\n",
    "```\n",
    "\n",
    "## Concatenate multiple devices - weekly report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
